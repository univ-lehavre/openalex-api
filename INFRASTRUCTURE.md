# Infrastructure Disponible - Cluster dirqual1-4

## üéØ R√©sum√© Ex√©cutif

Votre infrastructure de **4 serveurs identiques** d√©passe tr√®s largement les besoins du projet OpenAlex et offre une capacit√© de croissance exceptionnelle.

### Comparaison Besoins vs Disponible

| Ressource | Besoin OpenAlex | Disponible | Surplus |
|-----------|-----------------|------------|---------|
| **CPU C≈ìurs** | 32+ | **160** | **5√ó** |
| **CPU Threads** | 64+ | **320** | **5√ó** |
| **RAM** | 256 Go | **1 To** | **4√ó** |
| **SSD Rapide** | 13,5 To | **13,6 To** | ‚úÖ **Parfait** |
| **Stockage Total** | 15 To | **284 To** | **19√ó** |

**Verdict** : ‚úÖ Infrastructure **largement suffisante** pour h√©berger OpenAlex et bien plus.

---

## üìä Sp√©cifications D√©taill√©es par Serveur

### Configuration Identique (dirqual1, dirqual2, dirqual3, dirqual4)

```yaml
CPU:
  Mod√®le: Intel Xeon Silver 4316 @ 2.30-3.40GHz
  Sockets: 2 processeurs
  C≈ìurs physiques: 40 (20 √ó 2)
  Threads (vCPUs): 80
  Cache L3: 60 MiB
  Architecture: NUMA 2 n≈ìuds

M√©moire:
  Capacit√©: 252 Go (~256 Go)
  Type: DDR4 ECC
  Configuration: NUMA r√©parti

Stockage:
  NVMe SSD:
    - nvme0n1: 447 Go (syst√®me)
    - nvme1n1: 2,9 To (donn√©es)
    Total: 3,35 To

  HDD SAS:
    - 12 disques de 5,5 To chacun
    Total: 66 To (67,6 To r√©els)

R√©seau:
  OS: Debian
  Virtualisation: VT-x activ√©
```

### Totaux du Cluster (4 serveurs)

```yaml
Puissance de calcul:
  CPU Physiques: 160 c≈ìurs
  CPU Logiques: 320 threads
  Processeurs: 8√ó Intel Xeon Silver 4316

M√©moire:
  Total: 1 To (1 008 Go)
  Disponible: ~996 Go

Stockage:
  NVMe SSD: 13,6 To (haute performance)
  HDD SAS: 270 To (stockage masse)
  Total: 284 To
```

---

## üèóÔ∏è Architecture Recommand√©e

### R√©partition des R√¥les

#### üóÑÔ∏è Serveurs Base de Donn√©es (dirqual1-2)

**dirqual1** - PostgreSQL Primary
```yaml
R√¥le: Base de donn√©es principale
CPU: 40 c≈ìurs / 80 threads
RAM: 252 Go
Stockage:
  NVMe: 3,4 To ‚Üí PostgreSQL (3 To donn√©es + index)
  HDD RAID10: 33 To ‚Üí Backups quotidiens

Configuration PostgreSQL:
  shared_buffers: 64 Go
  effective_cache_size: 189 Go
  max_worker_processes: 40
  max_parallel_workers: 40
```

**dirqual2** - PostgreSQL Replica + Staging
```yaml
R√¥le: R√©plication + Blue-Green d√©ploiement
CPU: 40 c≈ìurs / 80 threads
RAM: 252 Go
Stockage:
  NVMe: 3,4 To ‚Üí Replica + Staging
  HDD RAID10: 33 To ‚Üí Archives + exports OpenAlex
```

#### üîç Serveur Recherche (dirqual3)

**dirqual3** - Elasticsearch + Redis
```yaml
R√¥le: Recherche plein texte et cache
CPU: 40 c≈ìurs / 80 threads
RAM: 252 Go
Stockage:
  NVMe: 3,4 To ‚Üí Elasticsearch (2 To) + Redis (100 Go)
  HDD RAID10: 33 To ‚Üí Snapshots + logs historiques

Configuration Elasticsearch:
  Heap Size: 31 Go (50% des 64 Go allou√©s)
  Shards: 5 primaires, 1 replica
  Index: Works + Authors
```

#### ‚öôÔ∏è Serveur Services (dirqual4)

**dirqual4** - API + Monitoring + CI/CD
```yaml
R√¥le: Services applicatifs et infrastructure
CPU: 40 c≈ìurs / 80 threads
RAM: 252 Go
Stockage:
  NVMe: 3,4 To ‚Üí FastAPI + Monitoring + CI/CD
  HDD RAID10: 33 To ‚Üí Disaster Recovery + snapshots

Services:
  - FastAPI (pods multiples)
  - Prometheus + Grafana + Loki
  - GitLab CI/CD
  - ETL temporaire
  - R√©serve: 2 To NVMe disponible
```

---

## üíæ Configuration Stockage RAID

### RAID 10 Recommand√© (Performance + Redondance)

**Par serveur** :
```bash
# 12 disques de 5,5 To en RAID 10
Capacit√© brute: 66 To
Capacit√© utilisable: 33 To (50% overhead)
Performance:
  - Lecture: Excellente (striping)
  - √âcriture: Excellente (mirroring)
Tol√©rance aux pannes: Jusqu'√† 6 disques (1 par miroir)

# Commande cr√©ation RAID
mdadm --create /dev/md0 --level=10 --raid-devices=12 \
  /dev/sd{a,b,c,d,e,f,g,h,i,j,k,l}

# Format XFS (optimal pour PostgreSQL)
mkfs.xfs -f /dev/md0
```

**Total cluster** :
- 132 To utilisables (33 To √ó 4)
- 270 To bruts
- Haute performance lecture/√©criture

---

## üìà Capacit√© de Croissance

### Ce que vous pouvez h√©berger

| Projet | Volume | Faisabilit√© |
|--------|--------|-------------|
| **OpenAlex complet** | 3 To | ‚úÖ Facile (10% capacit√© NVMe) |
| **10√ó OpenAlex** | 30 To | ‚úÖ Possible (NVMe + HDD) |
| **Dev + Staging + Prod** | 9 To | ‚úÖ Facile (3√ó environnements) |
| **20 ans de backups** | 60 To | ‚úÖ Possible (HDD RAID) |
| **Projets additionnels** | Variable | ‚úÖ Large marge disponible |

### Sc√©narios d'utilisation

#### Sc√©nario 1: OpenAlex uniquement
```
Utilisation:
  NVMe: 13,5 To / 13,6 To (99%)
  HDD: 30 To / 132 To (23%)

R√©serve disponible:
  HDD: 102 To pour expansion future
```

#### Sc√©nario 2: Multi-tenant (3 projets)
```
Projet 1 (OpenAlex): 13,5 To
Projet 2 (autre dataset): 10 To
Projet 3 (R&D): 5 To
Backups: 40 To

Total utilis√©: 68,5 To / 145,6 To (47%)
R√©serve: 77 To disponible
```

#### Sc√©nario 3: OpenAlex + ML/AI
```
OpenAlex: 13,5 To
Mod√®les ML: 20 To
Datasets training: 30 To
R√©sultats: 15 To

Total: 78,5 To / 145,6 To (54%)
R√©serve: 67 To
```

---

## üîß Scripts d'Inventaire

### V√©rification rapide de tous les serveurs

```bash
#!/bin/bash
# check-cluster.sh - V√©rification rapide du cluster

echo "=== INVENTAIRE CLUSTER dirqual1-4 ==="
echo ""

for server in dirqual{1..4}; do
  echo "=== $server ==="
  ssh $server './scripts/system-info.sh --json' | jq '{
    cpu: .cpu.physical_cores,
    ram_gb: .memory.total_gb,
    nvme_to: (.storage.nvme_gb / 1024),
    hdd_to: (.storage.hdd_gb / 1024)
  }'
  echo ""
done

echo "=== TOTAUX CLUSTER ==="
echo "CPU: 160 c≈ìurs physiques"
echo "RAM: 1 To"
echo "NVMe: 13,6 To"
echo "HDD: 270 To"
```

### Export CSV pour documentation

```bash
# G√©n√©rer rapport CSV complet
echo "Hostname,CPU_Cores,CPU_Threads,RAM_GB,NVMe_TB,HDD_TB,Total_TB" > cluster-inventory.csv

for server in dirqual{1..4}; do
  ssh $server './scripts/system-info.sh --csv' | tail -1 >> cluster-inventory.csv
done

# Afficher tableau format√©
column -t -s',' cluster-inventory.csv
```

---

## ‚úÖ Checklist de Pr√©paration

### Avant d√©ploiement Kubernetes

- [ ] V√©rifier connectivit√© r√©seau entre les 4 serveurs
- [ ] Configurer RAID 10 sur les 12 HDD de chaque serveur
- [ ] Installer Kubernetes sur tous les n≈ìuds
- [ ] Configurer storage classes pour NVMe et HDD
- [ ] Tester failover entre dirqual1 et dirqual2
- [ ] Configurer monitoring (node-exporter sur chaque serveur)
- [ ] Valider NUMA affinity pour PostgreSQL
- [ ] Configurer Huge Pages (32768 pages de 2 Mo)

### Tests de performance

- [ ] Benchmark disques NVMe (fio)
- [ ] Benchmark RAID 10 (fio)
- [ ] Test r√©seau inter-serveurs (iperf3)
- [ ] Test CPU (sysbench)
- [ ] Test RAM (memtest)
- [ ] Test PostgreSQL (pgbench)
- [ ] Test Elasticsearch (esrally)

---

## üìû Contacts et Support

### Scripts disponibles

- `scripts/system-info.sh` - Inventaire mat√©riel automatique
- `scripts/README.md` - Documentation des scripts
- `scripts/DEBIAN_FIXES.md` - Corrections Debian sp√©cifiques

### Documentation

- [Inventaire mat√©riel d√©taill√©](docs/06-kubernetes/hardware-inventory.md)
- [Strat√©gie de stockage](docs/01-stockage/strategy.md)
- [Vue d'ensemble du projet](docs/00-introduction/overview.md)

---

**Derni√®re mise √† jour** : 2026-01-12
**Infrastructure v√©rifi√©e** : dirqual1, dirqual2, dirqual3, dirqual4
**Status** : ‚úÖ Pr√™te pour d√©ploiement OpenAlex
**√âquipe** : Infrastructure - Universit√© Le Havre Normandie
