# Notes d'Infrastructure - PrÃ©cisions Importantes

## Architecture Stockage RÃ©elle

### Configuration NVMe

**IMPORTANT** : Les disques NVMe ne sont **PAS** utilisÃ©s pour les donnÃ©es applicatives.

| Disque | Taille | Usage | Serveur |
|--------|--------|-------|---------|
| nvme0n1 | 447 GB | **SystÃ¨me d'exploitation** | Tous (dirqual1-4) |
| nvme1n1 | 2.9 TB | **MÃ©tadonnÃ©es Ceph (OSD metadata)** | Tous (dirqual1-4) |

**Total NVMe** : 3.4 TB Ã— 4 = 13.6 TB
**Usage applicatif direct** : âš ï¸ **AUCUN**

### Configuration HDD

| Disques | Taille Unitaire | QuantitÃ© | Total Brut | Usage |
|---------|-----------------|----------|------------|-------|
| HDD SAS | 5.5 TB | 12 disques/serveur | 67 TB/serveur | **Toutes les donnÃ©es applicatives** |
| **Total cluster** | - | 48 disques | **270 TB brut** | PostgreSQL, Neo4j, InfluxDB, Elasticsearch |

### Architecture Ceph RÃ©visÃ©e

```text
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  Serveur dirqual1-4                    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  nvme0n1 (447 GB)  â†’ OS (Debian)                       â”‚
â”‚  nvme1n1 (2.9 TB)  â†’ Ceph OSD Metadata                 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  12Ã— HDD SAS (5.5 TB chacun) â†’ Ceph OSD Data          â”‚
â”‚    â”œâ”€ PostgreSQL data                                  â”‚
â”‚    â”œâ”€ Neo4j data                                       â”‚
â”‚    â”œâ”€ InfluxDB data                                    â”‚
â”‚    â”œâ”€ Elasticsearch data                               â”‚
â”‚    â””â”€ Redis snapshots                                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Implications pour les Performances

#### âš ï¸ Impact Performance

**Avant (supposÃ© NVMe pour donnÃ©es)** :
- Latence lecture : 0.1ms (NVMe)
- IOPS : 500K+ (NVMe)
- DÃ©bit : 3-7 GB/s (NVMe)

**RÃ©alitÃ© (HDD pour donnÃ©es)** :
- Latence lecture : 5-10ms (HDD 7200 RPM)
- IOPS : 100-200 (HDD)
- DÃ©bit : 150-250 MB/s (HDD)

**Facteur de diffÃ©rence** : 50-100Ã— plus lent en latence, 10-20Ã— en dÃ©bit

#### âœ… Avantages du HDD

1. **CapacitÃ© massive** : 270 TB vs 13.6 TB
2. **CoÃ»t par TB** : 10-20Ã— moins cher que NVMe
3. **DurabilitÃ©** : Meilleure pour charges de travail read-heavy
4. **Parfait pour** :
   - DonnÃ©es froides (anciennes publications)
   - Archives et backups
   - DonnÃ©es peu consultÃ©es

#### âŒ Limitations du HDD

1. **Latence Ã©levÃ©e** : 5-10ms vs 0.1ms (NVMe)
2. **IOPS limitÃ©s** : 100-200 vs 500K+ (NVMe)
3. **ProblÃ©matique pour** :
   - RequÃªtes transactionnelles frÃ©quentes (PostgreSQL OLTP)
   - Recherches complexes nÃ©cessitant random access
   - Workloads write-heavy

### StratÃ©gies d'Optimisation

#### 1. Utiliser les MÃ©tadonnÃ©es NVMe Ceph

Ceph peut stocker les mÃ©tadonnÃ©es OSD sur NVMe pour accÃ©lÃ©rer les opÃ©rations :
- **BlueStore metadata** sur nvme1n1
- **Write-Ahead Log (WAL)** sur nvme1n1
- **DonnÃ©es (data)** sur HDD

**Gain** : 30-50% amÃ©lioration latence d'Ã©criture

```yaml
# CephCluster configuration
storage:
  nodes:
    - name: dirqual1
      devices:
        # Data sur HDD
        - name: sda
          config:
            deviceClass: hdd
        # ... 11 autres HDD

        # Metadata sur NVMe
        - name: nvme1n1
          config:
            metadataDevice: true
            deviceClass: nvme-meta
```

#### 2. Augmenter les Caches

##### PostgreSQL
```ini
# postgresql.conf
shared_buffers = 64GB         # Compenser latence HDD
effective_cache_size = 180GB  # 70% de RAM disponible
work_mem = 512MB              # Tris en mÃ©moire
maintenance_work_mem = 8GB    # VACUUM, indexes
```

##### Neo4j
```conf
# neo4j.conf
dbms.memory.pagecache.size=48G
dbms.memory.heap.initial_size=16G
dbms.memory.heap.max_size=16G
```

##### Elasticsearch
```yaml
# elasticsearch.yml
indices.memory.index_buffer_size: 40%
indices.fielddata.cache.size: 30%
```

##### InfluxDB
```yaml
# influxdb.conf
cache-max-memory-size: 16GB
cache-snapshot-memory-size: 50MB
```

#### 3. RAID Configuration Optimale

Pour HDD, RAID 10 est crucial :

```bash
# 12 HDD par serveur
# RAID 10 : 6 paires mirrorÃ©es, stripÃ©es
# CapacitÃ© utilisable : 6Ã— 5.5 TB = 33 TB par serveur

# Avantages RAID 10 :
# - Lecture : 2Ã— performance (stripe)
# - Ã‰criture : Pas de pÃ©nalitÃ© (vs RAID 5/6)
# - TolÃ©rance : Perte de 1 disque par paire
```

**Total cluster avec RAID 10** :
- 4 serveurs Ã— 33 TB = **132 TB utilisables**
- RÃ©plication Ceph 3Ã— : **44 TB effectifs**

#### 4. Partitionnement Chaud/Froid

**DonnÃ©es chaudes (< 2 ans)** :
- Ceph pool avec plus de rÃ©plicas (3Ã—)
- Cache Redis agressif
- PrioritÃ© placement sur HDD rapides

**DonnÃ©es froides (> 2 ans)** :
- Ceph pool avec compression
- Moins de rÃ©plicas (2Ã—)
- Lecture occasionnelle acceptable

```yaml
# Pool chaud (rÃ©cent)
apiVersion: ceph.rook.io/v1
kind: CephBlockPool
metadata:
  name: hot-pool
spec:
  replicated:
    size: 3
  parameters:
    compression_mode: none
  deviceClass: hdd

---
# Pool froid (archives)
apiVersion: ceph.rook.io/v1
kind: CephBlockPool
metadata:
  name: cold-pool
spec:
  replicated:
    size: 2
  parameters:
    compression_mode: aggressive
    compression_algorithm: zstd
  deviceClass: hdd
```

### CapacitÃ©s RÃ©visÃ©es

#### Stockage Disponible avec RAID 10 + RÃ©plication Ceph

| Configuration | Brut | RAID 10 | Ceph 3Ã— | Ceph 2Ã— | Utilisable |
|---------------|------|---------|---------|---------|------------|
| **HDD Total** | 270 TB | 135 TB | 45 TB | 67.5 TB | **45-67 TB** |
| **Par serveur** | 67 TB | 33 TB | 11 TB | 16.5 TB | **11-16 TB** |

#### Allocation RecommandÃ©e (45 TB disponibles, rÃ©plication 3Ã—)

| Base de DonnÃ©es | Volume | RÃ©plication | Usage Brut | Marge |
|----------------|--------|-------------|------------|-------|
| **PostgreSQL** | 1.4 TB | 3Ã— | 4.2 TB | âœ… |
| **Neo4j** | 610 GB | 3Ã— | 1.8 TB | âœ… |
| **InfluxDB** | 170 GB | 3Ã— | 510 GB | âœ… |
| **Elasticsearch** | 1.3 TB | 2Ã— | 2.6 TB | âœ… |
| **Redis snapshots** | 64 GB | 3Ã— | 192 GB | âœ… |
| **Backups** | - | 2Ã— | 5 TB | âœ… |
| **Logs** | - | 2Ã— | 1 TB | âœ… |
| **TOTAL** | **3.5 TB** | - | **~15 TB** | **30 TB libres** |

**Verdict** : âœ… CapacitÃ© largement suffisante mÃªme avec HDD uniquement

### Performances Attendues RÃ©alistes

#### RequÃªtes Simples (GET by ID)

| OpÃ©ration | Avec NVMe (supposÃ©) | Avec HDD (rÃ©el) | DiffÃ©rence |
|-----------|---------------------|-----------------|------------|
| **Cache hit (Redis)** | 1-2ms | 1-2ms | = |
| **Cache miss â†’ DB** | 5-10ms | 15-30ms | **2-3Ã— plus lent** |
| **Avec buffer cache** | 5ms | 10ms | **2Ã— plus lent** |

#### RequÃªtes Complexes

| Type de RequÃªte | NVMe | HDD | Impact |
|-----------------|------|-----|--------|
| **Full table scan** | 500ms | 2-5s | **4-10Ã— plus lent** |
| **Index scan** | 50ms | 150-300ms | **3-6Ã— plus lent** |
| **AgrÃ©gations lourdes** | 2s | 10-20s | **5-10Ã— plus lent** |
| **Graphes Neo4j** | 10ms | 30-100ms | **3-10Ã— plus lent** |

#### âœ… Mitigations Efficaces

1. **Cache Redis** : 80-90% hit rate â†’ MajoritÃ© des requÃªtes < 5ms
2. **Buffers DB** : 70-80% pages en RAM â†’ Ã‰vite lecture HDD
3. **Vues matÃ©rialisÃ©es** : PrÃ©-calcul agrÃ©gations â†’ Pas de scan
4. **Partitionnement temporel** : Scans limitÃ©s aux partitions rÃ©centes

### Recommandations Finales

#### âœ… Architecture ValidÃ©e

L'architecture polyglotte reste optimale **mÃªme avec HDD** :

1. **Neo4j** : Graphes en mÃ©moire, HDD pour persistence
2. **InfluxDB** : Compression 92%, lectures sÃ©quentielles (friendly HDD)
3. **PostgreSQL** : Buffers 64 GB + cache Redis
4. **Elasticsearch** : Index en mÃ©moire, HDD pour segments

#### ğŸ¯ Objectifs de Performance RÃ©visÃ©s

| MÃ©trique | Objectif Initial (NVMe) | Objectif RÃ©aliste (HDD) |
|----------|-------------------------|-------------------------|
| **P50 latency** | < 100ms | **< 200ms** |
| **P95 latency** | < 500ms | **< 1s** |
| **P99 latency** | < 1s | **< 2s** |
| **Throughput** | 500 req/s | **200-300 req/s** |
| **Cache hit rate** | 70% | **80-90% (critique)** |

#### ğŸš€ Optimisations Critiques

1. **Cache Redis agressif** : 128 GB au lieu de 64 GB
2. **Buffers DB maximaux** : 70% RAM disponible
3. **Vues matÃ©rialisÃ©es** : Toutes agrÃ©gations frÃ©quentes
4. **Partitionnement** : DonnÃ©es chaudes/froides strict
5. **RAID 10** : Performance maximale HDD

---

## Conclusion

**L'infrastructure HDD est suffisante** pour l'API OpenAlex avec :
- âœ… CapacitÃ© : 45 TB disponibles vs 3.5 TB nÃ©cessaires (12Ã— surplus)
- âš ï¸ Performance : 2-10Ã— plus lent que NVMe mais compensable
- âœ… Cache : 80-90% hit rate â†’ MajoritÃ© requÃªtes rapides
- âœ… CoÃ»t : Optimal pour 270 TB de stockage

**Facteurs de succÃ¨s** :
1. Cache Redis bien dimensionnÃ© (128-256 GB)
2. Buffers DB gÃ©nÃ©reux (64-128 GB)
3. RAID 10 pour performances HDD
4. MÃ©tadonnÃ©es Ceph sur NVMe (nvme1n1)
5. Partitionnement chaud/froid

---

**Auteur** : Ã‰quipe Infrastructure - UniversitÃ© Le Havre Normandie
**Date** : 2026-01-12
**Version** : 1.0.0
